<!DOCTYPE html>
<html>
  <head>
    <title>
      Deep Learning Final Project
    </title>
    <link rel="stylesheet" href="styles.css" />
  </head>
<body>
  
<h1>Predicting Fight Damage in Slay the Spire</h1>
<p>
  Slay the Spire is a deck-building video game where you face rounds of monsters, upgrading and swapping out cards in your deck as you go.
  For my final project, I extended a mod for Slay the Spire that predicts the best action to take at each point in the game. 
  This is a turn-based game with deck-building, buying, and action phases, so the model is capable of recommending the best card to play during fights and best new card to choose during buy phases. 
  I found an existing mod that does exactly this, but I wanted to experiment with embeddings, batch sizes, and dropout layers as well as train the model on a new dataset released after the original mod was published.
  The new dataset was much larger, and I only ended up being able to use 5 million out of 75 million runs. 
  The <a href="https://colab.research.google.com/drive/19a0pjaS2iETqIHvagc_aOsJqEkIPD-WC?usp=sharing">new model I tuned</a> allowed for less overfitting compared to the original model, but it had a higher loss overall.
  I was also able to win a game using only actions recommended by the mod, which was really cool to see!
</p>

<center>
  <video width="1280" height="720" controls>
    <source src="Video.mp4" type="video/mp4">
  Your browser does not support the video tag.
  </video>
</center>

<h2>Problem Statement</h2>

<p>
The overall problem is to improve on the Slay-I mod for Slay the Spire to better predict damage taken during fights. 
The first goal is to reduce overfitting, an issue that the original author mentioned, by adding embedding, experimenting with batch size, and adding dropout layers.
The second goal is to preprocess a new dataset and train the original model on it, comparing training loss over number of epochs with the model trained on the original dataset.

</p>

<h2>Related Work</h2>

<p>
This project is based on the <a href="https://steamcommunity.com/sharedfiles/filedetails/?id=2157144906">Slay-I mod</a>, created by Alex Driedger. 
He has published the <a href="https://github.com/alexdriedger/SlayTheSpireFightPredictor">code for the model</a>, as well as the
 <a href="https://github.com/alexdriedger/SlayTheSpireFightPredictorMod">code to load the saved model</a> into the mod.
He walks through the project in this <a href=https://towardsdatascience.com/bringing-deep-neural-networks-to-slay-the-spire-a2971d5a5115>article</a>, which was where I got the inspiration for this project.

His work is composed of three parts: a data processing script to convert run history files into training samples, a Colab notebook to train the actual model, 
and Java code that put the saved model into a mod and integrated it with game UI. The second part, training the model itself, was what I wanted to focus on.
</p>

<p>
  The original datasets used for training and testing the mod were the <a href="https://spirelogs.com/">Spire Logs</a> dataset and the <a href="https://www.youtube.com/user/JoINrbs">Jorbs</a> dataset. 
  Spire Logs is a website where users can upload their own runs to see analytics, and all data uploaded is made public as well. 
  A "run" is one playthrough of the game, from start until the player dies, which spans multiple floors. 
  A training sample is information about the player on a given floor, including their deck at that point, relics held, monsters, and starting health.
  This dataset provided 325,000 training samples. The other one the original mod used was data personally provided by Jorbs, a streamer who excels at Slay the Spire.
  This dataset provided 2,000 training samples.
</p>

<p>
  A couple months after the original mod was written, the creators of Slay the Spire published a huge collection of 
  <a href="https://www.reddit.com/r/slaythespire/comments/jt5y1w/77_million_runs_an_sts_metrics_dump/">over 75 million runs</a>. 
  I struggled with resource limits and was only able to use 5 million of these, which ended up as around 1 million training samples after filtering out corrupted or out of date runs.
</p>

<h2>Methodology</h2>

<p>
  The creator of the model mentioned future improvements to be attempting to counteract overfitting and also training on more diverse data.
  
  First, I trained the original model to get a baseline train and test accuracy to compare with. As the original author mentioned, I saw signs of overfitting, so I experimented with various ways to reduce this.
  I tried adding embeddings for cards and relics, varying the batch size, and adding dropout layers.
  Once I arrived at a model less prone to overfitting, I trained it on 5 million runs from the new dataset.
</p>


<h2>Experiments</h2>

<p>
  To get a baseline, I trained the original model over 20 epochs and noticed a sign of overfitting: the training loss decreased on a nice curve, but validation loss stopped decreasing much after around 5 epochs.
  <center>
    <img src="base.png " alt="Training and Validation Loss for Original Model">
  </center>
</p>

<p>
  As the original author mentioned, I thought adding embeddings would be helpful, and wanted to see how batch size and dropout layers affected the validation loss.
  I decided to run these three experiments on the original dataset (Spire Logs) so that I could compare loss curves directly with the original model quickly.
  After finishing these experiments, I ran the model on the new dataset (results section) to determine whether or not it had been improved.
</p>


<h3>Adding Embedding</h3>
<p>
  The goal of embedding is to tranform sparse one-hot encoded vectors into an embedding matrix. This is helpful in cases with large dictionaries (like English words), which is not a concern here. 
  However, it also helps the model learn relationships between inputs, which is especially relevant for Slay the Spire as certain cards/relics synergize well and upgraded cards should be associated with each other.
  After adding embeddings, I hoped to see the validation loss over epochs get closes to the training loss as compared to the base model. 
  As shown below, this is what happened, which was an encouraging sign that introducing embeddings had potentially decreased overfitting.
</p>

<center>
  <img src="embeddings.png" alt="Training and Validation Loss Using Embeddings">
</center>

<p>
  However, when I compared the training and validation losses directly to those of the base model, I found that after introducing embeddings, both losses had gone up.
  I'm not sure why this might be, but as you can see below, the training loss flattened out very early compared to the base model.
  I thought this might potentially be helped by training the model on more data as my understanding is that embeddings do better with large amounts of data, 
  so I hoped the embedding would produce better results on the new dataset (in results section).
</p>

<center>
  <img src="embeddings_training.png" alt="Training Loss Using Embeddings">
  <img src="embeddings_validation.png" alt="Validation Loss Using Embeddings">
</center>


<h3>Varying Batch Size</h3>

<p>
  Since the validation loss was very noisy, I wanted to see the effect of varying batch size on training and validation loss.

</p>
<center>
  <img src="batch_training.png" alt="Training Loss When Varying Batch Size">
  <img src="batch_validation.png " alt="Validation Loss When Varying Batch Size">
</center>

<p>
  As shown in the plots above, varying the batch size didn't cause any major shifts in stability. 
  The lower batch sizes (16, 32, and 64) had slightly better training and validation loss, so I chose to stick with a batch size of 32.  
</p>

<h3>Adding Dropout Layers</h3>

<p>
  The goal of the dropout layer is to stop some percentage of neurons from contributing to the next layer, which is meant to help lessen overfitting.
  The original network struture consists of an embedding layer, then a dense, dropout, and dense layer. 
  I didn't want to increase the first dropout layer too much as I was worried about it dropping a lot of the input, so I decided to add an extra dropout, dense pair at the end.
  Below are the comparisons of training and validation loss before and after adding the extra dropout layer, with both droupouts set to 10%. 
  I trained for a shorter number of epochs because I wanted to quickly confirm that the layer was beneficial before experimenting with dropout rates.
  As shown below, the extra layer decreased training and validation loss slightly.
</p>

<center>
  <img src="layer_training.png " alt="Effect of Adding Another Dropout Layer (Training)">
  <img src="layer_validation.png " alt="Effect of Adding Another Dropout Layer (Validation)">
</center>

<p>
  Next, I experimented with dropout rates of 5-50% on the second dropout layer. I had thought that increasing the dropout rate would help with overfitting, but in fact saw the opposite: 
  5% and 10% had the lowest training loss and all dropouts seemed to perform similarly for validation loss. 
  This was not at all what I had expected, and at first I thought I had flipped the dropout order and made a mistake in plotting or training.
  However, after reviewing the code, my current hypothesis is that embedding mostly solved the overfitting problem, and therefore adding dropout on top of that wasn't especially helpful.
</p>

<center>
  <img src="dropout_training.png " alt="Effect of Varying Dropout Layer (Training)">
  <img src="dropout_validation.png " alt="Effect of Varying Dropout Layer (Validation)">
</center>

<h2>Results</h2>

<p>
  After experimenting with embeddings, batch sizes, and dropout on the Spire Logs dataset, I then got to try it out on the official dataset. 
  I ended up using 5 million runs (out of 75 million), which led to 1 million training samples, around 3 times more than Spire Logs.
  My final model had an embedding layer, a batch size of 32, and a droupout rate of 10%.
  The below images depict the results of training both the original model and my model on the new dataset.
</p>

<center>
  <img src="new_dataset_no_embed.png" alt="Original Model">
  <img src="new_dataset.png" alt="New Model">
</center>

<p>
  Although the original model reaches a lower training and validation loss, the validation loss doesn't decrease nicely with the training loss.
  This suggests that it wouldn't get much benefit from training over a larger amount of data, as is the goal.
  My model, on the right, has much smoother validation loss, which I would consider a success; however, the loss starts going up again around epoch 12, which is still a sign of overfitting.
</p>

<p>
  Overall, I met my goal of decreasing overfitting on the Spire Logs dataset by adding an embedding layer. 
  However, varying batch size and adding dropout didn't have significant effects I was expecting.  
  The final model with embedding still had very clear signs of overfitting as well, so I would say this project was partially succesful.
  Regardless, I learned a lot and had a lot of fun playing Slay the Spire to testmy model!
</p>

<h2>Demo</h2>

<p>
  The following demo goes through the main features of the model: card selection, fight damage prediction, and card upgrade.
  Disclaimer: the mod UI and framework was created by Alex Dreidger, my updates only changed the model that does the actual predictions behind the scenes.
</p>

<center>
  <video width="1280" height="720" controls>
    <source src="Demo.mp4" type="video/mp4">
  Your browser does not support the video tag.
  </video>
</center>


<h2>Future Work</h2>

<p>
  Although my goal was to reduce overfitting and train the improved model on a giant dataset, I wasn't able to fully fulfil this aim due to resource limits and a lack of experience working with embeddings.
  The next big step would be to train on all 75 million runs of the new dataset. 
  I only used a subset of these to keep things manageable while I experimented with other network parameters, but I would want to make use of all the data available to further better the model.
  More experiments should be done with embeddings as well, as I suspect something else can be done to reduce overfitting.  
  We could potentially try using k-fold cross validation as well to help with overfitting.
</p>
<p>
  In addition to just making the model perform better, I thought of a couple new features that might be useful to add.
  Just like we evaluate cards based on fight damage reduced, we could evaluate relics in a similar way and present this information when choosing relics in the shop or after a boss fight.
  Having the model adapt to your playstyle by incorporating your own data as you play would also be a fun avenue to explore.
  Finally, there are tools made to visualize semantically similar words based on an embedding matrix, and it would be interesting to see this adapted to visualize cards/relics that synergize well.
</p>

</body>
<footer></footer>
</html>